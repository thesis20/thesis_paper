\section{Conclusion}\label{sec:conclusion}
In this paper, we have proposed two ways to include context and side-information in graph convolution neural networks.
While there are not many competing methods available for comparison in context-specific recommendation scenarios, we have shown that compared to both traditional factorization machines and deep learning versions of those, the CSGCN models outperform them across most datasets, ranging from an improvement of $70.66\%$ to $510.0\%$.
On the Frappe dataset the CSGCN models are outperformed by $84.45\%$ to $1473.37\%$.
The large fluctuations in improvements and decreases are likely caused by the evaluation methodology where the models attempt to predict a single relevant item amongst every item in the dataset, as well as the small size of the Frappe dataset causing learning difficulties.
Likewise, we have shown how the context can be aggregated and used in a general recommendation scenario with competitive results compared to state-of-the-art methods.
The CSGCN models improve performance in a range of $0.07\%$ to $10.01\%$, while being outperformed on Frappe once again and on the precision metric for ML1M, in a range of $0.04\%$ to $72.62\%$.
The large decrease on Frappe is caused by the data split, which inadvertently facilitated the simple baseline Top Pop.
Some items in Frappe are so popular that almost every 13th interaction in the dataset is with these items.
While this is a perfect scenario for TopPop, it has proven to be a big challenge for more advanced methods such as the CSGCN models.

% Skriv noget om at future work kunne være info på edges? Evt hvordan vi har prøvet at implementere det
% Double training objective
%\todo[color=red]{x - Conclude upon the fun times}

\subsection{Future work}
The most prominent future work is further contemplation on how to express context in a way that is compatible with the GCN models.
Our proposal is to look into ways to represent context as edge information in a feasible way.
While writing this paper, we attempted to model it in a way such that there was an embedding for each (user,item,context) tuple.
However, this quickly proved infeasible since the number of embeddings explodes with just a few contexts available.
\\
The CSGCN models make use of negative sampling for calculating loss.
While this is a regular way to handle BPR loss, it is known to be biased \cite{nonsampling,NegativeSampling}, and sensitive to the number of negative samples \cite{NCF}.
Fluctuations in the sampled negative item can cause difficulties in converging, and even sampling more negative items can increase performance.\\
Chen et al. \cite{nonsampling} propose to learn FM models without negative sampling to improve ranking performance for context-aware recommendation, and increase stability due to considering all samples in each parameter update.
However, using negative sampling can be necessary since using non-sampling not only requires the method to look at each observed interaction from the dataset, but also to consider each non-observed interaction as a negative data point.
Because of this consideration, CSGCN-IS uses negative sampling for training, and, due to employing BPR loss, only one negative sample is used.
For future work on the methods, it may be interesting to look into other ways to calculate the loss that are less reliant on hitting good samples.
\\
Additionally, throughout the experiments, it shows that contexts are equal in terms of importance.
Due to this, it could be beneficial to include an attention mechanism that is able to learn the importance of each context or context combination for the convolution layer.
This kind of attention idea could also be applied to layers.
As we increase the amount of layers in a GCN, more noise will be introduced to the convolutions.
To counter this, it may be beneficial to look into more advanced weighting mechanisms for layers, such that nodes further away from the root node are less influential in the layer combination operation.
\\
Finally, both CSGCN methods are based on the simplified models of GCNs which are gaining popularity.
For this paper, we have performed limited experimentation on extending this with various components from traditional GCNs, such as re-adding linear transformation and non-linear activation, without success.
In recent months, several new papers on GCNs have been published which present new extensions to GCNs.
These extensions include new layer aggregation functions and various implementations of knowledge graphs, which may be of interest to improve the performance of the context-aware models presented in this paper.
