\section{Preliminaries}\label{sec:preliminaries}
%In this paper, we present two GCN-based context-aware models, which take side-information into account in order to alleviate the sparsity problem often found in CARS.
This section will present some preliminary knowledge and defintions pertaining to the definition of context and side-information, the components of a GCN, how to represent data, and higher-order connectivity in graphs.

\subsection{Defining side-information and context}\label{subsec:define_context_sideinfo}
Datasets used for RS are often very sparse, since not every user will interact with every available item, leading to learning issues.
Sparsity issues are exacerbated in CARS due to the addition of contextual dimensions that users only interact with a few of \cite{SparsityCARS}.
The use of side-information can help alleviate problems with CF in a sparse situation where the users and items do not have many interactions \cite{KGAT}.
While GCN-based RS perform well on simple recommendation tasks that include only user-item interactions \cite{NGCF,LightGCN}, combining them with the utilization of context is largely unexplored to the best of our knowledge.
However, side-information has been used in some GCN-based RS such as KGAT through the use of knowledge graphs \cite{KGAT}.\\
A common problem when researching CARS is that context and side-information are vaguely defined or have overlapping definitions in various papers.
For clarity, the following are the definitions of context and side-information used in this paper:
\\\\
\textbf{Context}:
The context definition used in this paper is identical to the one found in \cite{contextDefinition}, where \textit{context is any information that can be used to characterize the situation of an entity. An entity is a person, place, or object that is considered relevant to the interaction between a user and an application, including the user and the applications themselves.}
An entity in the case of an RS is either a user or an item to be recommended.
The context would thus be information about the conditions at the time of the interaction between the user and the item, meaning it would be associated with the interaction.
For example, a Christmas movie may be more likely to be a relevant recommendation during the winter months than in the summer, and a bar may be a better recommendation on a Saturday night than a Tuesday morning.\\
Throughout this paper, we will refer to context in the three following levels of granularity:\\\\
All possible context values, $C$, e.g.:
$$C = \{spring, \cdots, winter\} \cup \{morning, \cdots, midnight\}$$
Context values for a given interaction, $C'$, e.g.:
$$C' = \{winter, midnight\}$$
A single context value, $c$, e.g. 
$$c = winter$$
\textbf{Side-information}:
The side-information definition used is based on the description in \cite{SideInfoDefinition}, where different types of side-information include information such as user networks, user profiles, and item descriptors.
For users, this could be information like their age or occupation, which may have an influence on their preferences.
For items, it is information about the item such as the genre of a movie, which may be helpful since it can help capture user preferences, such as a user displaying a preference for action movies.
% For side-information, the term \textit{field} is used to describe, for example, the category \textit{location}. Each field has a number of values, known as \textit{features}, where a value could be \textit{Denmark} for the field \textit{location}.
% Side-information can thus be represented as one-hot or multi-hot feature vectors, depending on whether or not a field can have one or multiple features.
% \begin{equation}
%     x = [0,0,0,1,0,1]
%     \label{eq:multi-hot-example}
% \end{equation}
% \Cref{eq:multi-hot-example} shows an example of a multi-hot vector representing the field \textit{genre}, showing that it possesses 2 of the 6 possible features (genres).

\subsection{The components of a GCN}
GCNs are variants of convolutional neural networks (CNN) used on graph-based data \cite{KOrderConnectivity}.
GCNs learn the feature representations of each node in a graph through multiple graph convolution layers.
For layer $l$ of the graph convolution, the node representations that were output from the previous layer $l-1$ are used as input.
For each layer, the representations of the nodes in the graph are aggregated with the representations of their neighbor nodes.
A nonlinear activation function is also traditionally applied at each layer to introduce non-linearity to the model \cite{KOrderConnectivity}.
Representations are propagated with information about their neighbors in each graph convolution layer through three steps: feature propagation, linear transformation, and nonlinear activation.
Feature propagation averages features in the local neighborhood of a node, such that the representation of each node becomes an aggregation of its neighbors' features.
This is done for each node in the graph.
The next step is to use a linear transformation, such as $x_1 = x_0w_0$ where the embedding $x_0$ is updated by multiplying it with a weight $w_0$, thus becoming $x_1$.
Doing this allows the network to find the best fitting set of parameters through the linear transformation by training through optimizing a loss function \cite{RecentConvNets}.
Finally, a non-linear activation function such as \texttt{ReLU} is applied to the output, allowing it to better fit real-world datasets.
\\\\
While traditional GCN models contain all three components, several papers \cite{SimplifyingGCN,LightGCN,HeteGCN} suggest that this is largely caused by the history of GCNs.
Wu et al. present the idea that while most machine learning algorithms followed a clear path from an initial simple model to a more complex model through the needs of more expressivity, this was not the case of GCNs \cite{SimplifyingGCN}.
Instead, they were proposed in the recent years of neural networks where deep learning had gained high popularity, and as such GCNs were built directly on top of a complex model.
To investigate whether this was useful, they present simple graph convolution (SGC), a simplified version of GCNs that could have preceded GCNs if the traditional path had been taken.
This led them to remove the linear transformation and nonlinear activation, to reach a simplified linear model.
Interestingly, they proved that not only does this not degrade performance of the models, it generally matches or improves both in terms of performance and speed \cite{SimplifyingGCN}.\\
This signifies that the expressive power of GCNs originates primarily from propagation in the convolution layers, rather than the nonlinear feature extractions we know from traditional GCNs \cite{SimplifyingGCN}.
\\
Having defined GCNs, their applicability for RS tasks is now examined in terms of data and its representation.

\subsection{Types of data for RS}
Data used for RS can be classified as either implicit or explicit \cite{oard1998implicit}.
Explicit data is provided by the user of the RS, such as a user providing a rating of an item.
Data provided explicitly by users is commonly used for the score prediction task, where, as with traditional matrix factorization (MF) approaches, a score matrix is reconstructed in order to generate missing ratings for users.
Implicit data, on the other hand, is not explicitly provided by the user, but rather collected through their behavior.
A user clicking a specific item on a web-shop or deciding to watch a specific movie would be implicit data, defining an interaction between the user and the item.
Since explicit data requires explicit user involvement, it is usually more scarce than implicit data.
A problem with implicit data, however, is that it does not directly encode whether or not the user had a preference for the given item as is possible with explicit data, only that they interacted with it.
A common prediction task for implicit data is top-$k$ recommendation, where a list of $k$ items is produced for the user.

\subsection{Data representation}
In graph-based CF RS, data is typically represented in the form of a bipartite graph as seen on \Cref{fig:bipartite-graph}.
\begin{figure}[h]
\begin{tikzpicture}[thick,
    every node/.style={draw,circle},
    fsnode/.style={fill=myblue},
    ssnode/.style={fill=myred},
    every fit/.style={ellipse,draw,inner sep=-2pt,text width=2cm},
    shorten >= 3pt,shorten <= 3pt
  ]
  
  % the vertices of U
  \begin{scope}[start chain=going below,node distance=7mm]
  \foreach \i in {u1,u2,u3}
    \node[fsnode,on chain] (f\i) [label=left: \i] {};
  \end{scope}
  
  % the vertices of I
  \begin{scope}[xshift=4cm,start chain=going below,node distance=7mm]
  \foreach \i in {i1,i2,i3,i4}
    \node[ssnode,on chain] (s\i) [label=right: \i] {};
  \end{scope}
  
  % the set U
  \node [myblue,fit=(fu1) (fu3),label=above:$U$] {};
  % the set I
  \node [myred,fit=(si1) (si4),label=above:$I$] {};
  
  % the edges
  \draw (fu1) -- (si1);
  \draw (fu1) -- (si4);
  \draw (fu2) -- (si1);
  \draw (fu2) -- (si3);
  \draw (fu3) -- (si3);
  \draw (fu3) -- (si2);
\end{tikzpicture}
\caption{Example of a bipartite graph.}
\label{fig:bipartite-graph}
\end{figure}
This bipartite graph consists of two distinct sets of nodes: Users $U$ and items $I$, and a set of undirected edges $E$ that connect the user and item if they have interacted with each other.
This graph can also be represented as an adjacency matrix, which is useful for the implementation of a graph convolutional layer as it allows for the implementation of graph convolution through matrix multiplication between the adjacency matrix and the embeddings of the graph nodes.
This is the case since the values being multiplied with the representation are from the adjacency matrix that defines the neighbor nodes, thus aggregating the neighborhood.\\
This could, however, cause issues in terms of exploding or vanishing gradients, leading to scenarios where the method ends up unable to learn.
To mitigate this, a degree matrix consisting of the summation of row entries in the diagonal can be employed in order to normalize the entries in the adjacency matrix by the number of neighbors, to ensure the gradients do not grow too large or small.
The graph seen in \Cref{fig:bipartite-graph} can be extended to also include nodes that represent side-information, which are connected to the user and item nodes.
If side-information is represented as nodes in the graph, they can be included in the adjacency matrix and the graph convolution such that information can be passed from these nodes as well.
Higher-order connectivity can be gained by repeatedly doing convolution operations in multiple layers, meaning information is passed from neighboring nodes further away from the central node \cite{SimplifyingGCN, KOrderConnectivity}, based on the number of layers.
a two layer GCN would, for example, be able to pass information from nodes two steps from the central node, allowing the collaborative signal to be influenced by nodes that are further from the central node.\\\\
With this preliminary knowledge in place, we will look at the two proposed models in the following sections.
